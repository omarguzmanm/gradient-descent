{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 0\n",
    "\n",
    "**Name:** Omar Alejandro Guzmán Munguía\n",
    "\n",
    "**e-mail:** omar.guzman5063@alumnos.udg.mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory on the Gradient Descent algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Matyas function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, I am going to use the Matyas function, which is a commonly used function to evaluate optimization algorithms like Gradient Descent.\n",
    "\n",
    "The first step is to define the function. This function will receive two parameters (x, y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Matyas function\n",
    "def f_matyas(x, y):\n",
    "    return 0.26 * (x**2 + y**2) - 0.48 * x * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define the gradient of the Matyas function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the gradient of the Matyas function\n",
    "def grad_f_matyas(x, y):\n",
    "    grad_x = 0.52 * x - 0.48 * y\n",
    "    grad_y = 0.52 * y - 0.48 * x\n",
    "    return np.array([grad_x, grad_y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gradient Descent function requires four parameters to operate effectively. I am going to explain each one:\n",
    "- **starting_point:** a tuple representing the initial coordinates (x, y) where the descent begins.\n",
    "- **learning_rate:** a float that defines the step size, determining how much the parameters are updated in each iteration.\n",
    "- **max_iterations:** an integer specifying the maximum number of iterations to prevent infinite loops.\n",
    "- **convergence_treshold:** a float that sets the stopping condition, ensuring the process halts when updates become sufficiently small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the Gradient Descent process is pretty straightforward once we have the right parameters.\n",
    "\n",
    "First, we break down the starting point, which is a tuple, into `x and y` so we can use them individually. We also create an array to track the trajectory of the points as they move toward the minimum. Then, we loop through the specified number of `iterations`, calculating the gradient using `grad_f_matyas(x, y) `to determine the direction of movement. Using the learning rate, we update x and y, controlling the step size to ensure steady progress. Each updated value is stored to keep track of the descent. After every update, we check if the change in x and y is small enough to meet the `convergence threshold`, allowing us to stop early if we’re close to the minimum. Once the loop finishes, we return the optimized values along with the trajectory of updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local minimum found at: (x, y) = (0.09084654767794724, 0.09084654767794724)\n",
      "Function value at minimum: 0.00033012380900006126\n"
     ]
    }
   ],
   "source": [
    "# Gradient Descent Algorithm\n",
    "def gradient_descent(starting_point, learning_rate, max_iterations, convergence_threshold):\n",
    "    x, y = starting_point\n",
    "    history = []  # To store the trajectory of (x, y)\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        grad = grad_f_matyas(x, y)\n",
    "        x_new = x - learning_rate * grad[0]\n",
    "        y_new = y - learning_rate * grad[1]\n",
    "\n",
    "        # Store the current point\n",
    "        history.append((x, y))\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm([x_new - x, y_new - y]) < convergence_threshold:\n",
    "            print(f\"Converged after {i} iterations.\")\n",
    "            break\n",
    "\n",
    "        # Update (x, y)\n",
    "        x, y = x_new, y_new\n",
    "\n",
    "    return x, y, history\n",
    "\n",
    "# Parameters\n",
    "starting_point = (5, 5)  \n",
    "learning_rate = 0.1\n",
    "max_iterations = 1000\n",
    "convergence_threshold = 1e-6\n",
    "\n",
    "# Run Gradient Descent\n",
    "x_min, y_min, history = gradient_descent(starting_point, learning_rate, max_iterations, convergence_threshold)\n",
    "\n",
    "# Results\n",
    "print(f\"Local minimum found at: (x, y) = ({x_min}, {y_min})\")\n",
    "print(f\"Function value at minimum: {f_matyas(x_min, y_min)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
